{
  "instance_id": "zombar__tunnelmesh-365",
  "repo": "zombar/tunnelmesh",
  "base_commit": "b7e2bfabcd4e4902be9ba6095cc76b6e87f0f8ea",
  "version": "unknown",
  "created_at": "2026-02-13T21:16:18.000202+00:00",
  "problem_statement": "**Bug Report: Erasure Coding Support in S3 PutObject()**\n\n**Bug Report: Erasure Coding Support in S3 PutObject()**\n\n**Component Affected:** S3 PutObject() Method\n\n**Description of the Problem:** The current implementation of the S3 PutObject() method does not support erasure coding for file uploads, leading to reduced fault tolerance and potential data loss in case of failures during storage. When a file is uploaded that exceeds 100MB, it is processed using standard replication, which may not provide the optimal balance between data integrity and storage overhead, especially for larger files. As a result, users may experience increased storage costs and a lack of protection against data corruption or loss when storing large objects.",
  "patch": "diff --git a/.gitignore b/.gitignore\nindex 8dc39a5..dc62e32 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -2,6 +2,7 @@\n bin/\n /tunnelmesh\n /tunnelmesh-server\n+/tunnelmesh-s3bench\n *.exe\n *.exe~\n *.dll\n@@ -54,8 +55,8 @@ id_rsa.pub\n # Log files\n *.log\n \n-# Dynamic service discovery targets (generated by sd_generator)\n-monitoring/targets/peers.json\n+# Dynamic service discovery targets (generated by sd_generator and runtime)\n+monitoring/targets/*.json\n \n # Temporary files\n tmp/\ndiff --git a/internal/coord/replication/chunk_registry.go b/internal/coord/replication/chunk_registry.go\nindex 0ca485d..6447c70 100644\n--- a/internal/coord/replication/chunk_registry.go\n+++ b/internal/coord/replication/chunk_registry.go\n@@ -557,7 +557,7 @@ func (cr *ChunkRegistry) CleanupStaleOwners(maxAge time.Duration) int {\n \n // RegisterShardChunk registers a chunk as an erasure coding shard.\n // This is used when storing parity shards or data shards with tracking metadata.\n-func (cr *ChunkRegistry) RegisterShardChunk(hash string, size int64, replicationFactor int, shardType string, shardIndex int, parentFileID string) error {\n+func (cr *ChunkRegistry) RegisterShardChunk(hash string, size int64, parentFileID string, shardType string, shardIndex int, replicationFactor int) error {\n \tif hash == \"\" {\n \t\treturn fmt.Errorf(\"chunk hash cannot be empty\")\n \t}\ndiff --git a/internal/coord/s3/store.go b/internal/coord/s3/store.go\nindex 33b3153..02f7205 100644\n--- a/internal/coord/s3/store.go\n+++ b/internal/coord/s3/store.go\n@@ -32,6 +32,11 @@ import (\n // For large file uploads: Consider 24 hours or more\n const GCGracePeriod = 1 * time.Hour\n \n+// MaxErasureCodingFileSize is the maximum file size for erasure coding (Phase 1).\n+// Files larger than this will use standard replication.\n+// Streaming encoder (Phase 6) will remove this limit.\n+const MaxErasureCodingFileSize = 100 * 1024 * 1024 // 100 MB\n+\n // ErasureCodingPolicy defines the erasure coding configuration for a bucket.\n type ErasureCodingPolicy struct {\n \tEnabled      bool `json:\"enabled\"`       // Whether erasure coding is enabled for new objects\n@@ -148,6 +153,9 @@ type ChunkRegistryInterface interface {\n \t// RegisterChunkWithReplication registers a chunk with a custom replication factor\n \tRegisterChunkWithReplication(hash string, size int64, replicationFactor int) error\n \n+\t// RegisterShardChunk registers an erasure-coded shard chunk with shard metadata\n+\tRegisterShardChunk(hash string, size int64, parentFileID, shardType string, shardIndex, replicationFactor int) error\n+\n \t// UnregisterChunk removes the local coordinator as an owner of the chunk\n \tUnregisterChunk(hash string) error\n \n@@ -178,6 +186,7 @@ type Store struct {\n \tversionRetentionDays    int                    // Days to retain object versions (0 = forever)\n \tmaxVersionsPerObject    int                    // Max versions to keep per object (0 = unlimited)\n \tversionRetentionPolicy  VersionRetentionPolicy\n+\terasureCodingSemaphore  chan struct{} // Limits concurrent erasure coding operations (memory safety)\n \tmu                      sync.RWMutex\n }\n \n@@ -190,9 +199,10 @@ func NewStore(dataDir string, quota *QuotaManager) (*Store, error) {\n \t}\n \n \tstore := &Store{\n-\t\tdataDir: dataDir,\n-\t\tquota:   quota,\n-\t\tlogger:  zerolog.Nop(), // Default to no-op logger\n+\t\tdataDir:                dataDir,\n+\t\tquota:                  quota,\n+\t\tlogger:                 zerolog.Nop(),           // Default to no-op logger\n+\t\terasureCodingSemaphore: make(chan struct{}, 10), // Allow 10 concurrent EC operations\n \t}\n \n \t// Calculate initial quota usage from existing objects\n@@ -725,6 +735,317 @@ func (s *Store) updateBucketSize(bucketName string, delta int64) error {\n \treturn s.writeBucketMeta(bucketName, bucketMeta)\n }\n \n+// putObjectWithErasureCoding stores an object using Reed-Solomon erasure coding.\n+// The entire file is buffered in memory, encoded into data+parity shards, then stored.\n+// Data shards are CDC chunked (preserves deduplication), parity shards are stored directly.\n+//\n+// NOTE: This is Phase 1 implementation with full buffering. Streaming encoder will be added in Phase 6.\n+//\n+//nolint:gocyclo // Complexity will be reduced when streaming encoder is added (Phase 6)\n+func (s *Store) putObjectWithErasureCoding(ctx context.Context, bucket, key string, reader io.Reader, size int64, contentType string, metadata map[string]string, bucketMeta *BucketMeta) (*ObjectMeta, error) {\n+\t// Caller must hold s.mu.Lock()\n+\n+\tk := bucketMeta.ErasureCoding.DataShards\n+\tm := bucketMeta.ErasureCoding.ParityShards\n+\n+\t// Validate shard configuration (defense against malicious/corrupted bucket metadata)\n+\t// While validateErasureCodingPolicy already checked this, re-validate at upload time\n+\t// to prevent issues if metadata was corrupted or modified externally\n+\tif k < 1 || k > 32 || m < 1 || m > 32 || k+m > 64 {\n+\t\treturn nil, fmt.Errorf(\"invalid erasure coding config: k=%d, m=%d (max 32 each, 64 total)\", k, m)\n+\t}\n+\n+\t// Acquire semaphore to limit concurrent erasure coding operations (memory safety)\n+\t// Each operation buffers up to 100MB, so limiting concurrency prevents OOM\n+\tselect {\n+\tcase s.erasureCodingSemaphore <- struct{}{}:\n+\t\tdefer func() { <-s.erasureCodingSemaphore }()\n+\tcase <-ctx.Done():\n+\t\treturn nil, fmt.Errorf(\"waiting for erasure coding slot: %w\", ctx.Err())\n+\t}\n+\n+\tmetaPath := s.objectMetaPath(bucket, key)\n+\n+\t// Check if object already exists (for quota update calculation and versioning)\n+\tvar oldSize int64\n+\tif oldMeta, err := s.getObjectMeta(bucket, key); err == nil {\n+\t\toldSize = oldMeta.Size\n+\t}\n+\n+\t// Check quota if configured (only if object is growing)\n+\tif s.quota != nil && size > oldSize {\n+\t\tdelta := size - oldSize\n+\t\tif !s.quota.CanAllocate(delta) {\n+\t\t\treturn nil, ErrQuotaExceeded\n+\t\t}\n+\t}\n+\n+\t// Archive current version for version history\n+\tif err := s.archiveCurrentVersion(bucket, key); err != nil {\n+\t\treturn nil, fmt.Errorf(\"archive current version: %w\", err)\n+\t}\n+\n+\t// Create parent directories for metadata\n+\tif err := os.MkdirAll(filepath.Dir(metaPath), 0755); err != nil {\n+\t\treturn nil, fmt.Errorf(\"create meta dir: %w\", err)\n+\t}\n+\n+\t// Buffer entire file into memory (required for Reed-Solomon encoding)\n+\tdata, err := io.ReadAll(io.LimitReader(reader, size))\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"read file data: %w\", err)\n+\t}\n+\tif int64(len(data)) != size {\n+\t\treturn nil, fmt.Errorf(\"file size mismatch: expected %d bytes, got %d\", size, len(data))\n+\t}\n+\n+\t// Generate version ID before encoding (needed for tracking shards)\n+\tversionID := generateVersionID()\n+\n+\t// Check for cancellation before expensive encoding operation\n+\tselect {\n+\tcase <-ctx.Done():\n+\t\treturn nil, fmt.Errorf(\"context canceled before encoding: %w\", ctx.Err())\n+\tdefault:\n+\t}\n+\n+\t// Encode file into Reed-Solomon shards\n+\tdataShards, parityShards, err := EncodeFile(data, k, m)\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"encode file with erasure coding (k=%d,m=%d,size=%d): %w\", k, m, size, err)\n+\t}\n+\n+\t// Calculate shard size for metadata\n+\tshardSize := int64(0)\n+\tif len(dataShards) > 0 && len(dataShards[0]) > 0 {\n+\t\tshardSize = int64(len(dataShards[0]))\n+\t}\n+\n+\tnow := time.Now().UTC()\n+\tcoordID := s.coordinatorID\n+\n+\t// File-level version vector\n+\tfileVersionVector := make(map[string]uint64)\n+\tif coordID != \"\" {\n+\t\tfileVersionVector[coordID] = 1\n+\t}\n+\n+\t// Track all chunks (data shard chunks + parity shards)\n+\tvar chunks []string\n+\tchunkMetadata := make(map[string]*ChunkMetadata)\n+\tvar dataHashes []string // Original CDC chunk hashes from data shards\n+\tvar parityHashes []string\n+\n+\t// Cleanup on failure: remove all written chunks from CAS and registry\n+\t// This prevents orphaned data that would waste space until GC runs\n+\tvar success bool\n+\tdefer func() {\n+\t\tif !success && len(chunks) > 0 {\n+\t\t\t// Best-effort cleanup - errors are logged but not propagated\n+\t\t\tfor _, hash := range chunks {\n+\t\t\t\tif err := s.cas.DeleteChunk(context.Background(), hash); err != nil {\n+\t\t\t\t\ts.logger.Warn().Str(\"hash\", hash[:8]).Err(err).Msg(\"failed to cleanup chunk during rollback\")\n+\t\t\t\t}\n+\t\t\t\tif s.chunkRegistry != nil {\n+\t\t\t\t\tif err := s.chunkRegistry.UnregisterChunk(hash); err != nil {\n+\t\t\t\t\t\ts.logger.Warn().Str(\"hash\", hash[:8]).Err(err).Msg(\"failed to unregister chunk during rollback\")\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}()\n+\n+\t// Process data shards: chunk them using CDC to preserve deduplication\n+\t// Each data shard is treated as a separate \"mini-file\" that gets chunked\n+\tmd5Hasher := md5.New()\n+\tfor i, shard := range dataShards {\n+\t\t// Hash the original data for ETag calculation\n+\t\tmd5Hasher.Write(shard)\n+\n+\t\t// Chunk this data shard using CDC\n+\t\tshardChunker := NewStreamingChunker(bytes.NewReader(shard))\n+\t\tfor {\n+\t\t\tchunk, chunkHash, err := shardChunker.NextChunk()\n+\t\t\tif errors.Is(err, io.EOF) {\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\tif err != nil {\n+\t\t\t\treturn nil, fmt.Errorf(\"chunk data shard %d/%d (versionID=%s): %w\", i, k, versionID, err)\n+\t\t\t}\n+\n+\t\t\t// Write chunk to CAS\n+\t\t\tif _, err := s.cas.WriteChunk(ctx, chunk); err != nil {\n+\t\t\t\treturn nil, fmt.Errorf(\"write data shard %d/%d chunk %s (versionID=%s): %w\", i, k, chunkHash[:8], versionID, err)\n+\t\t\t}\n+\n+\t\t\t// Register chunk ownership\n+\t\t\tif s.chunkRegistry != nil {\n+\t\t\t\t// Register as data shard chunk\n+\t\t\t\tif err := s.chunkRegistry.RegisterShardChunk(chunkHash, int64(len(chunk)), versionID, \"data\", i, bucketMeta.ReplicationFactor); err != nil {\n+\t\t\t\t\tif os.Getenv(\"DEBUG\") != \"\" || os.Getenv(\"TUNNELMESH_DEBUG\") != \"\" {\n+\t\t\t\t\t\tfmt.Fprintf(os.Stderr, \"chunk registry warning: failed to register data shard chunk %s: %v\\n\", chunkHash[:8], err)\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\t// Track chunk metadata\n+\t\t\tversionVector := make(map[string]uint64)\n+\t\t\tif coordID != \"\" {\n+\t\t\t\tversionVector[coordID] = 1\n+\t\t\t}\n+\t\t\tvar owners []string\n+\t\t\tif coordID != \"\" {\n+\t\t\t\towners = []string{coordID}\n+\t\t\t}\n+\n+\t\t\tchunkMetadata[chunkHash] = &ChunkMetadata{\n+\t\t\t\tHash:          chunkHash,\n+\t\t\t\tSize:          int64(len(chunk)),\n+\t\t\t\tVersionVector: versionVector,\n+\t\t\t\tOwners:        owners,\n+\t\t\t\tFirstSeen:     now,\n+\t\t\t\tLastModified:  now,\n+\t\t\t\tShardType:     \"data\",\n+\t\t\t\tShardIndex:    i,\n+\t\t\t\tParentFileID:  versionID,\n+\t\t\t}\n+\n+\t\t\tchunks = append(chunks, chunkHash)\n+\t\t\tdataHashes = append(dataHashes, chunkHash)\n+\t\t}\n+\t}\n+\n+\t// Process parity shards: store directly without CDC chunking\n+\t// (parity shards are already optimally sized and don't benefit from dedup)\n+\tfor i, shard := range parityShards {\n+\t\t// Write parity shard directly to CAS (it will compute SHA-256 hash)\n+\t\tparityHash, err := s.cas.WriteChunk(ctx, shard)\n+\t\tif err != nil {\n+\t\t\treturn nil, fmt.Errorf(\"write parity shard %d/%d (versionID=%s): %w\", i, m, versionID, err)\n+\t\t}\n+\n+\t\t// Register parity shard ownership\n+\t\tif s.chunkRegistry != nil {\n+\t\t\tif err := s.chunkRegistry.RegisterShardChunk(parityHash, int64(len(shard)), versionID, \"parity\", i, bucketMeta.ReplicationFactor); err != nil {\n+\t\t\t\tif os.Getenv(\"DEBUG\") != \"\" || os.Getenv(\"TUNNELMESH_DEBUG\") != \"\" {\n+\t\t\t\t\tfmt.Fprintf(os.Stderr, \"chunk registry warning: failed to register parity shard %s: %v\\n\", parityHash[:8], err)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\t// Track parity shard metadata\n+\t\tversionVector := make(map[string]uint64)\n+\t\tif coordID != \"\" {\n+\t\t\tversionVector[coordID] = 1\n+\t\t}\n+\t\tvar owners []string\n+\t\tif coordID != \"\" {\n+\t\t\towners = []string{coordID}\n+\t\t}\n+\n+\t\tchunkMetadata[parityHash] = &ChunkMetadata{\n+\t\t\tHash:          parityHash,\n+\t\t\tSize:          int64(len(shard)),\n+\t\t\tVersionVector: versionVector,\n+\t\t\tOwners:        owners,\n+\t\t\tFirstSeen:     now,\n+\t\t\tLastModified:  now,\n+\t\t\tShardType:     \"parity\",\n+\t\t\tShardIndex:    i,\n+\t\t\tParentFileID:  versionID,\n+\t\t}\n+\n+\t\tchunks = append(chunks, parityHash)\n+\t\tparityHashes = append(parityHashes, parityHash)\n+\t}\n+\n+\t// Generate ETag from MD5 hash of original data (S3-compatible format)\n+\thash := md5Hasher.Sum(nil)\n+\tetag := fmt.Sprintf(\"\\\"%s\\\"\", hex.EncodeToString(hash))\n+\n+\t// Update quota tracking\n+\tvar quotaUpdated bool\n+\tif s.quota != nil {\n+\t\tif oldSize > 0 {\n+\t\t\ts.quota.Update(bucket, oldSize, size)\n+\t\t} else {\n+\t\t\ts.quota.Allocate(bucket, size)\n+\t\t}\n+\t\tquotaUpdated = true\n+\t}\n+\n+\t// Create object metadata with erasure coding info\n+\tobjMeta := ObjectMeta{\n+\t\tKey:           key,\n+\t\tSize:          size, // Original file size (before encoding)\n+\t\tContentType:   contentType,\n+\t\tETag:          etag,\n+\t\tLastModified:  now,\n+\t\tMetadata:      metadata,\n+\t\tVersionID:     versionID,\n+\t\tChunks:        chunks,\n+\t\tChunkMetadata: chunkMetadata,\n+\t\tVersionVector: fileVersionVector,\n+\t\tErasureCoding: &ErasureCodingInfo{\n+\t\t\tEnabled:      true,\n+\t\t\tDataShards:   k,\n+\t\t\tParityShards: m,\n+\t\t\tShardSize:    shardSize,\n+\t\t\tDataHashes:   dataHashes,\n+\t\t\tParityHashes: parityHashes,\n+\t\t},\n+\t}\n+\n+\t// Set expiry if configured (skip for system bucket)\n+\tif s.defaultObjectExpiryDays > 0 && bucket != SystemBucket {\n+\t\texpiry := now.AddDate(0, 0, s.defaultObjectExpiryDays)\n+\t\tobjMeta.Expires = &expiry\n+\t}\n+\n+\t// Serialize and write metadata\n+\tmetaData, err := json.MarshalIndent(objMeta, \"\", \"  \")\n+\tif err != nil {\n+\t\t// Rollback quota on failure\n+\t\tif quotaUpdated {\n+\t\t\tif oldSize > 0 {\n+\t\t\t\ts.quota.Update(bucket, size, oldSize)\n+\t\t\t} else {\n+\t\t\t\ts.quota.Release(bucket, size)\n+\t\t\t}\n+\t\t}\n+\t\treturn nil, fmt.Errorf(\"marshal object meta: %w\", err)\n+\t}\n+\n+\tif err := syncedWriteFile(metaPath, metaData, 0644); err != nil {\n+\t\t// Rollback quota on failure\n+\t\tif quotaUpdated {\n+\t\t\tif oldSize > 0 {\n+\t\t\t\ts.quota.Update(bucket, size, oldSize)\n+\t\t\t} else {\n+\t\t\t\ts.quota.Release(bucket, size)\n+\t\t\t}\n+\t\t}\n+\t\treturn nil, fmt.Errorf(\"write object meta: %w\", err)\n+\t}\n+\n+\t// Prune expired versions (lazy cleanup)\n+\ts.pruneExpiredVersions(ctx, bucket, key)\n+\n+\t// Update bucket size\n+\tsizeDelta := size - oldSize\n+\tif sizeDelta != 0 {\n+\t\tif err := s.updateBucketSize(bucket, sizeDelta); err != nil {\n+\t\t\t// Log error but don't fail the put operation\n+\t\t\t_ = err\n+\t\t}\n+\t}\n+\n+\t// Mark as successful to prevent cleanup rollback\n+\tsuccess = true\n+\n+\treturn &objMeta, nil\n+}\n+\n // PutObject writes an object to a bucket using CDC chunks stored in CAS.\n // Archives the current version before overwriting (for version history).\n //\n@@ -763,6 +1084,18 @@ func (s *Store) PutObject(ctx context.Context, bucket, key string, reader io.Rea\n \t// Get bucket's replication factor for chunk registration\n \treplicationFactor := bucketMeta.ReplicationFactor\n \n+\t// Check if erasure coding is enabled and file size is suitable\n+\tuseErasureCoding := bucketMeta.ErasureCoding != nil &&\n+\t\tbucketMeta.ErasureCoding.Enabled &&\n+\t\tsize > 0 &&\n+\t\tsize <= MaxErasureCodingFileSize\n+\n+\tif useErasureCoding {\n+\t\t// Use erasure coding path (buffer entire file)\n+\t\treturn s.putObjectWithErasureCoding(ctx, bucket, key, reader, size, contentType, metadata, bucketMeta)\n+\t}\n+\n+\t// Use standard streaming path for non-erasure-coded objects or large files\n \tmetaPath := s.objectMetaPath(bucket, key)\n \n \t// Check if object already exists (for quota update calculation and versioning)\n",
  "test_patch": "diff --git a/internal/coord/replication/chunk_registry_test.go b/internal/coord/replication/chunk_registry_test.go\nindex c99ac72..c2f58b3 100644\n--- a/internal/coord/replication/chunk_registry_test.go\n+++ b/internal/coord/replication/chunk_registry_test.go\n@@ -516,7 +516,7 @@ func TestRegisterShardChunk(t *testing.T) {\n \tregistry := NewChunkRegistry(\"coord-1\", nil)\n \n \t// Register a parity shard\n-\terr := registry.RegisterShardChunk(\"parity-hash-1\", 1024, 2, \"parity\", 0, \"file-version-123\")\n+\terr := registry.RegisterShardChunk(\"parity-hash-1\", 1024, \"file-version-123\", \"parity\", 0, 2)\n \tif err != nil {\n \t\tt.Fatalf(\"RegisterShardChunk failed: %v\", err)\n \t}\n@@ -548,7 +548,7 @@ func TestRegisterShardChunkInvalidType(t *testing.T) {\n \tregistry := NewChunkRegistry(\"coord-1\", nil)\n \n \t// Register with invalid shard type\n-\terr := registry.RegisterShardChunk(\"hash-1\", 1024, 2, \"invalid\", 0, \"file-version-123\")\n+\terr := registry.RegisterShardChunk(\"hash-1\", 1024, \"file-version-123\", \"invalid\", 0, 2)\n \tif err == nil {\n \t\tt.Error(\"expected error for invalid shard type, got nil\")\n \t}\n@@ -558,7 +558,7 @@ func TestRegisterShardChunkDataShard(t *testing.T) {\n \tregistry := NewChunkRegistry(\"coord-1\", nil)\n \n \t// Register a data shard\n-\terr := registry.RegisterShardChunk(\"data-hash-1\", 2048, 2, \"data\", 5, \"file-version-456\")\n+\terr := registry.RegisterShardChunk(\"data-hash-1\", 2048, \"file-version-456\", \"data\", 5, 2)\n \tif err != nil {\n \t\tt.Fatalf(\"RegisterShardChunk failed: %v\", err)\n \t}\n@@ -581,14 +581,14 @@ func TestGetParityShardsForFile(t *testing.T) {\n \tregistry := NewChunkRegistry(\"coord-1\", nil)\n \n \t// Register multiple shards for same file\n-\t_ = registry.RegisterShardChunk(\"data-0\", 1024, 2, \"data\", 0, \"file-v1\")\n-\t_ = registry.RegisterShardChunk(\"data-1\", 1024, 2, \"data\", 1, \"file-v1\")\n-\t_ = registry.RegisterShardChunk(\"parity-0\", 1024, 2, \"parity\", 0, \"file-v1\")\n-\t_ = registry.RegisterShardChunk(\"parity-1\", 1024, 2, \"parity\", 1, \"file-v1\")\n-\t_ = registry.RegisterShardChunk(\"parity-2\", 1024, 2, \"parity\", 2, \"file-v1\")\n+\t_ = registry.RegisterShardChunk(\"data-0\", 1024, \"file-v1\", \"data\", 0, 2)\n+\t_ = registry.RegisterShardChunk(\"data-1\", 1024, \"file-v1\", \"data\", 1, 2)\n+\t_ = registry.RegisterShardChunk(\"parity-0\", 1024, \"file-v1\", \"parity\", 0, 2)\n+\t_ = registry.RegisterShardChunk(\"parity-1\", 1024, \"file-v1\", \"parity\", 1, 2)\n+\t_ = registry.RegisterShardChunk(\"parity-2\", 1024, \"file-v1\", \"parity\", 2, 2)\n \n \t// Register shards for different file\n-\t_ = registry.RegisterShardChunk(\"other-parity\", 1024, 2, \"parity\", 0, \"file-v2\")\n+\t_ = registry.RegisterShardChunk(\"other-parity\", 1024, \"file-v2\", \"parity\", 0, 2)\n \n \t// Get parity shards for file-v1\n \tshards, err := registry.GetParityShardsForFile(\"file-v1\")\n@@ -638,10 +638,10 @@ func TestGetShardsForFile(t *testing.T) {\n \tregistry := NewChunkRegistry(\"coord-1\", nil)\n \n \t// Register mixed shards\n-\t_ = registry.RegisterShardChunk(\"data-0\", 1024, 2, \"data\", 0, \"file-v1\")\n-\t_ = registry.RegisterShardChunk(\"data-1\", 1024, 2, \"data\", 1, \"file-v1\")\n-\t_ = registry.RegisterShardChunk(\"parity-0\", 1024, 2, \"parity\", 0, \"file-v1\")\n-\t_ = registry.RegisterShardChunk(\"parity-1\", 1024, 2, \"parity\", 1, \"file-v1\")\n+\t_ = registry.RegisterShardChunk(\"data-0\", 1024, \"file-v1\", \"data\", 0, 2)\n+\t_ = registry.RegisterShardChunk(\"data-1\", 1024, \"file-v1\", \"data\", 1, 2)\n+\t_ = registry.RegisterShardChunk(\"parity-0\", 1024, \"file-v1\", \"parity\", 0, 2)\n+\t_ = registry.RegisterShardChunk(\"parity-1\", 1024, \"file-v1\", \"parity\", 1, 2)\n \n \t// Get all shards for file\n \tshards, err := registry.GetShardsForFile(\"file-v1\")\n@@ -683,10 +683,10 @@ func TestGetShardOwnersByType(t *testing.T) {\n \tregistry := NewChunkRegistry(\"coord-1\", nil)\n \n \t// Register various shards\n-\t_ = registry.RegisterShardChunk(\"data-0\", 1024, 2, \"data\", 0, \"file-v1\")\n-\t_ = registry.RegisterShardChunk(\"data-1\", 1024, 2, \"data\", 1, \"file-v1\")\n-\t_ = registry.RegisterShardChunk(\"parity-0\", 1024, 2, \"parity\", 0, \"file-v1\")\n-\t_ = registry.RegisterShardChunk(\"parity-1\", 1024, 2, \"parity\", 1, \"file-v1\")\n+\t_ = registry.RegisterShardChunk(\"data-0\", 1024, \"file-v1\", \"data\", 0, 2)\n+\t_ = registry.RegisterShardChunk(\"data-1\", 1024, \"file-v1\", \"data\", 1, 2)\n+\t_ = registry.RegisterShardChunk(\"parity-0\", 1024, \"file-v1\", \"parity\", 0, 2)\n+\t_ = registry.RegisterShardChunk(\"parity-1\", 1024, \"file-v1\", \"parity\", 1, 2)\n \n \t// Get parity shard owners\n \towners, err := registry.GetShardOwnersByType(\"parity\")\n@@ -732,10 +732,10 @@ func TestCleanupOrphanedShards(t *testing.T) {\n \tregistry := NewChunkRegistry(\"coord-1\", nil)\n \n \t// Register shards for multiple files\n-\t_ = registry.RegisterShardChunk(\"data-0\", 1024, 2, \"data\", 0, \"file-v1\")\n-\t_ = registry.RegisterShardChunk(\"parity-0\", 1024, 2, \"parity\", 0, \"file-v1\")\n-\t_ = registry.RegisterShardChunk(\"data-1\", 1024, 2, \"data\", 0, \"file-v2\")\n-\t_ = registry.RegisterShardChunk(\"parity-1\", 1024, 2, \"parity\", 0, \"file-v2\")\n+\t_ = registry.RegisterShardChunk(\"data-0\", 1024, \"file-v1\", \"data\", 0, 2)\n+\t_ = registry.RegisterShardChunk(\"parity-0\", 1024, \"file-v1\", \"parity\", 0, 2)\n+\t_ = registry.RegisterShardChunk(\"data-1\", 1024, \"file-v2\", \"data\", 0, 2)\n+\t_ = registry.RegisterShardChunk(\"parity-1\", 1024, \"file-v2\", \"parity\", 0, 2)\n \n \t// Register regular chunk (no parent file)\n \t_ = registry.RegisterChunk(\"regular-chunk\", 1024)\n@@ -776,8 +776,8 @@ func TestCleanupOrphanedShardsEmptyActive(t *testing.T) {\n \tregistry := NewChunkRegistry(\"coord-1\", nil)\n \n \t// Register shards\n-\t_ = registry.RegisterShardChunk(\"parity-0\", 1024, 2, \"parity\", 0, \"file-v1\")\n-\t_ = registry.RegisterShardChunk(\"parity-1\", 1024, 2, \"parity\", 1, \"file-v1\")\n+\t_ = registry.RegisterShardChunk(\"parity-0\", 1024, \"file-v1\", \"parity\", 0, 2)\n+\t_ = registry.RegisterShardChunk(\"parity-1\", 1024, \"file-v1\", \"parity\", 1, 2)\n \n \t// Cleanup with no active files\n \tcleaned := registry.CleanupOrphanedShards(map[string]bool{})\ndiff --git a/internal/coord/s3/distributed_reader_test.go b/internal/coord/s3/distributed_reader_test.go\nindex f367e15..fc85211 100644\n--- a/internal/coord/s3/distributed_reader_test.go\n+++ b/internal/coord/s3/distributed_reader_test.go\n@@ -63,6 +63,10 @@ func (m *mockChunkRegistry) RegisterChunkWithReplication(hash string, size int64\n \treturn nil\n }\n \n+func (m *mockChunkRegistry) RegisterShardChunk(hash string, size int64, parentFileID string, shardType string, shardIndex int, replicationFactor int) error {\n+\treturn nil\n+}\n+\n func (m *mockChunkRegistry) UnregisterChunk(hash string) error {\n \treturn nil\n }\ndiff --git a/internal/coord/s3/gc_distributed_test.go b/internal/coord/s3/gc_distributed_test.go\nindex 7482a30..29dda72 100644\n--- a/internal/coord/s3/gc_distributed_test.go\n+++ b/internal/coord/s3/gc_distributed_test.go\n@@ -297,6 +297,10 @@ func (f *failingChunkRegistry) RegisterChunkWithReplication(hash string, size in\n \treturn assert.AnError\n }\n \n+func (f *failingChunkRegistry) RegisterShardChunk(hash string, size int64, parentFileID string, shardType string, shardIndex int, replicationFactor int) error {\n+\treturn assert.AnError\n+}\n+\n func (f *failingChunkRegistry) UnregisterChunk(hash string) error {\n \treturn assert.AnError\n }\ndiff --git a/internal/coord/s3/integration_test.go b/internal/coord/s3/integration_test.go\nindex 82fb62c..cf9aae1 100644\n--- a/internal/coord/s3/integration_test.go\n+++ b/internal/coord/s3/integration_test.go\n@@ -280,6 +280,16 @@ func (r *threadSafeRegistry) RegisterChunkWithReplication(hash string, size int6\n \treturn nil\n }\n \n+func (r *threadSafeRegistry) RegisterShardChunk(hash string, size int64, parentFileID string, shardType string, shardIndex int, replicationFactor int) error {\n+\tr.mu.Lock()\n+\tdefer r.mu.Unlock()\n+\n+\tif r.owners[hash] == nil {\n+\t\tr.owners[hash] = []string{\"local\"}\n+\t}\n+\treturn nil\n+}\n+\n func (r *threadSafeRegistry) UnregisterChunk(hash string) error {\n \tr.mu.Lock()\n \tdefer r.mu.Unlock()\n",
  "hints_text": "",
  "environment_setup_commit": "b7e2bfabcd4e4902be9ba6095cc76b6e87f0f8ea",
  "install_config": {
    "python": "3.9",
    "install": "make build",
    "test_cmd": "TUNNELMESH_TEST=1 go test -short ./...",
    "pre_install": [
      "apt-get update",
      "apt-get install -y wget git gcc make",
      "wget -q https://go.dev/dl/go1.24.4.linux-amd64.tar.gz -O /tmp/go.tar.gz",
      "rm -rf /usr/local/go && tar -C /usr/local -xzf /tmp/go.tar.gz",
      "export PATH=/usr/local/go/bin:$PATH",
      "echo 'export PATH=/usr/local/go/bin:$HOME/go/bin:$PATH' >> ~/.bashrc"
    ],
    "pip_packages": [],
    "reqs_path": [],
    "env_yml_path": []
  },
  "meta": {
    "commit_name": "head_commit",
    "num_modified_files": 3,
    "has_test_patch": true,
    "is_lite": true,
    "llm_score": {
      "difficulty_score": null,
      "issue_text_score": null,
      "test_score": null
    }
  },
  "license_name": "AGPL-3.0",
  "FAIL_TO_PASS": [],
  "PASS_TO_PASS": [],
  "requirements": "",
  "environment": ""
}