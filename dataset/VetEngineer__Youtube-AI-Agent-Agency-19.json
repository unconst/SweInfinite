{
  "instance_id": "VetEngineer__Youtube-AI-Agent-Agency-19",
  "repo": "VetEngineer/Youtube-AI-Agent-Agency",
  "base_commit": "43a8aab546ef1f289da9d5f91618bdf00b7ccc67",
  "version": "unknown",
  "created_at": "2026-02-13T17:31:07.114159+00:00",
  "problem_statement": "[P7-1] \ube44\ub3d9\uae30 \uc791\uc5c5 \ud050 (Redis + Arq)\n\n## Stream A: \ube44\ub3d9\uae30 \uc791\uc5c5 \ud050\n\n**\ube0c\ub79c\uce58**: `feature/p7-1-async-task-queue`\n**\ub2f4\ub2f9**: Claude Code #1\n\n### \uc791\uc5c5 \ub0b4\uc6a9\n\n1. Redis + Arq \uae30\ubc18 worker \uc11c\ube44\uc2a4 \uad6c\ud604\n2. `POST /pipeline/run` \u2192 \ud050 enqueue \ubc29\uc2dd\uc73c\ub85c \uc804\ud658\n3. Worker\uac00 \ud30c\uc774\ud504\ub77c\uc778 \uc2e4\ud589, DB status \uc5c5\ub370\uc774\ud2b8\n4. docker-compose\uc5d0 Redis + Worker \uc11c\ube44\uc2a4 \ucd94\uac00\n5. \ud14c\uc2a4\ud2b8 \ubc0f `docs/QUEUES.md` \uc791\uc131\n\n### \uc18c\uc720 \ud30c\uc77c\n\n- `docker-compose.yml` \u2190 Primary Owner\n- `packages/agents/src/api/routes/pipeline.py`\n- `Makefile` (worker/redis \ud0c0\uac9f \ucd94\uac00)\n- NEW: `packages/agents/src/worker/` (tasks.py, config.py)\n- NEW: `docs/QUEUES.md`\n- `packages/agents/pyproject.toml` (queue \uc758\uc874\uc131 \uadf8\ub8f9 \ucd94\uac00)\n\n### pyproject.toml \ucd94\uac00 \uc139\uc158\n\n```toml\n[project.optional-dependencies]\nqueue = [\"arq>=0.26\", \"redis>=5.0\"]\n```\n\n### \uac80\uc99d\n\n- [ ] `make test` \ud1b5\uacfc\n- [ ] `make lint` \ud1b5\uacfc\n- [ ] Worker\uac00 \ud050\uc5d0\uc11c \uc791\uc5c5\uc744 \uac00\uc838\uc640 \uc2e4\ud589\ud558\ub294 \uac83 \ud655\uc778\n- [ ] `docker compose up`\uc73c\ub85c Redis + Worker \uc815\uc0c1 \uad6c\ub3d9\n\n### \ucc38\uace0\n\n- \uba38\uc9c0 \uc21c\uc11c: 4\ubc88\uc9f8 (\ub9c8\uc9c0\ub9c9)\n- \uad00\ub828 \ubb38\uc11c: `docs/PARALLEL_WORK_PLAN.md`",
  "patch": "diff --git a/Dockerfile b/Dockerfile\nindex d432116..d745b60 100644\n--- a/Dockerfile\n+++ b/Dockerfile\n@@ -11,7 +11,7 @@ COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv\n WORKDIR /app\n \n COPY packages/agents/pyproject.toml packages/agents/\n-RUN cd packages/agents && uv pip install --system -e \".[all]\"\n+RUN cd packages/agents && uv pip install --system -e \".[all,queue]\"\n \n COPY packages/agents/src packages/agents/src\n COPY channels channels\ndiff --git a/Makefile b/Makefile\nindex ffd0106..30eaf15 100644\n--- a/Makefile\n+++ b/Makefile\n@@ -1,4 +1,4 @@\n-.PHONY: help install test test-cov lint format run server clean docker-build docker-up docker-down docker-logs dev-setup db-migrate db-upgrade db-downgrade db-history rag-index\n+.PHONY: help install test test-cov lint format run server clean docker-build docker-up docker-down docker-logs dev-setup db-migrate db-upgrade db-downgrade db-history rag-index worker redis-up redis-down\n \n AGENTS_DIR = packages/agents\n \n@@ -73,3 +73,16 @@ dev-setup: install ## \uac1c\ubc1c \ud658\uacbd \ucd08\uae30\ud654\n \n rag-index: ## \ucc44\ub110 \ube0c\ub79c\ub4dc \uc790\ub8cc RAG \uc778\ub371\uc2f1 (channel= \ud544\uc218)\n \tcd $(AGENTS_DIR) && uv run python -c \"from src.brand_researcher.rag import BrandIndexer, RAGConfig; idx = BrandIndexer(RAGConfig()); print(f'Indexed {idx.index_channel(\\\"$(channel)\\\", __import__(\\\"pathlib\\\").Path(\\\"../../channels/$(channel)\\\"))} chunks')\"\n+\n+# ============================================\n+# Worker / Redis (P7-1)\n+# ============================================\n+\n+worker: ## Arq \uc6cc\ucee4 \uc2e4\ud589 (\ub85c\uceec)\n+\tcd $(AGENTS_DIR) && uv run python -m arq src.worker.tasks.WorkerConfig\n+\n+redis-up: ## Redis \ucee8\ud14c\uc774\ub108 \uc2dc\uc791\n+\tdocker compose up -d redis\n+\n+redis-down: ## Redis \ucee8\ud14c\uc774\ub108 \uc885\ub8cc\n+\tdocker compose stop redis\ndiff --git a/docker-compose.yml b/docker-compose.yml\nindex 093b05c..4ea729f 100644\n--- a/docker-compose.yml\n+++ b/docker-compose.yml\n@@ -15,9 +15,51 @@ services:\n     environment:\n       - PYTHONUNBUFFERED=1\n       - DATABASE_URL=postgresql+asyncpg://agency:${DB_PASSWORD:-localdevpassword}@db:5432/youtube_agency\n+      - REDIS_HOST=redis\n+      - REDIS_PORT=6379\n     depends_on:\n       db:\n         condition: service_healthy\n+      redis:\n+        condition: service_healthy\n+    restart: unless-stopped\n+\n+  worker:\n+    build:\n+      context: .\n+      dockerfile: Dockerfile\n+    container_name: youtube-ai-worker\n+    command: [\"python\", \"-m\", \"arq\", \"src.worker.tasks.WorkerConfig\"]\n+    volumes:\n+      - ./packages/agents/src:/app/packages/agents/src:ro\n+      - ./channels:/app/channels\n+      - ./output:/app/output\n+    env_file:\n+      - .env\n+    environment:\n+      - PYTHONUNBUFFERED=1\n+      - DATABASE_URL=postgresql+asyncpg://agency:${DB_PASSWORD:-localdevpassword}@db:5432/youtube_agency\n+      - REDIS_HOST=redis\n+      - REDIS_PORT=6379\n+    depends_on:\n+      db:\n+        condition: service_healthy\n+      redis:\n+        condition: service_healthy\n+    restart: unless-stopped\n+\n+  redis:\n+    image: redis:7-alpine\n+    container_name: youtube-ai-redis\n+    ports:\n+      - \"6379:6379\"\n+    volumes:\n+      - redisdata:/data\n+    healthcheck:\n+      test: [\"CMD\", \"redis-cli\", \"ping\"]\n+      interval: 10s\n+      timeout: 5s\n+      retries: 5\n     restart: unless-stopped\n \n   db:\n@@ -40,3 +82,4 @@ services:\n \n volumes:\n   pgdata:\n+  redisdata:\ndiff --git a/docs/QUEUES.md b/docs/QUEUES.md\nnew file mode 100644\nindex 0000000..367a2ae\n--- /dev/null\n+++ b/docs/QUEUES.md\n@@ -0,0 +1,101 @@\n+# \ube44\ub3d9\uae30 \uc791\uc5c5 \ud050 \uc544\ud0a4\ud14d\ucc98\n+\n+> Phase 7-1: Redis + Arq \uae30\ubc18 \ube44\ub3d9\uae30 \ud30c\uc774\ud504\ub77c\uc778 \uc2e4\ud589\n+\n+## \uac1c\uc694\n+\n+\ud30c\uc774\ud504\ub77c\uc778 \uc2e4\ud589\uc744 FastAPI `BackgroundTasks`\uc5d0\uc11c Redis \uae30\ubc18 Arq \uc791\uc5c5 \ud050\ub85c \uc804\ud658\ud569\ub2c8\ub2e4.\n+\uc774\ub97c \ud1b5\ud574 \uc6cc\ucee4 \ud504\ub85c\uc138\uc2a4\ub97c \ub3c5\ub9bd\uc801\uc73c\ub85c \uc2a4\ucf00\uc77c\ub9c1\ud558\uace0, \uc791\uc5c5 \uc7ac\uc2dc\ub3c4/\ubaa8\ub2c8\ud130\ub9c1\uc774 \uac00\ub2a5\ud574\uc9d1\ub2c8\ub2e4.\n+\n+## \uc544\ud0a4\ud14d\ucc98\n+\n+```\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  FastAPI API \u2502\u2500\u2500\u2500\u2500>\u2502  Redis  \u2502\u2500\u2500\u2500\u2500>\u2502  Arq Worker  \u2502\u2500\u2500\u2500\u2500>\u2502   DB   \u2502\n+\u2502  (enqueue)   \u2502     \u2502  Queue  \u2502     \u2502  (execute)   \u2502     \u2502        \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n+```\n+\n+1. `POST /api/v1/pipeline/run` \uc694\uccad\uc774 \ub4e4\uc5b4\uc624\uba74 DB\uc5d0 `pending` \uc0c1\ud0dc\ub85c \uc800\uc7a5\n+2. Redis\uac00 \uc0ac\uc6a9 \uac00\ub2a5\ud558\uba74 Arq \ud050\uc5d0 \uc791\uc5c5 \ub4f1\ub85d\n+3. Worker\uac00 \ud050\uc5d0\uc11c \uc791\uc5c5\uc744 \uac00\uc838\uc640 \ud30c\uc774\ud504\ub77c\uc778 \uc2e4\ud589\n+4. \uc2e4\ud589 \uacb0\uacfc\ub97c DB\uc5d0 \uc5c5\ub370\uc774\ud2b8 (`completed` / `failed`)\n+\n+## \ud3f4\ubc31 \uc804\ub7b5\n+\n+Redis\uac00 \uc0ac\uc6a9 \ubd88\uac00\ub2a5\ud558\uba74 \uae30\uc874 `BackgroundTasks` \ubc29\uc2dd\uc73c\ub85c \uc790\ub3d9 \ud3f4\ubc31\ub429\ub2c8\ub2e4.\n+\uc774\ub97c \ud1b5\ud574 \uac1c\ubc1c \ud658\uacbd\uc5d0\uc11c Redis \uc5c6\uc774\ub3c4 \uc815\uc0c1 \ub3d9\uc791\ud569\ub2c8\ub2e4.\n+\n+```python\n+# pipeline.py \ud575\uc2ec \ub85c\uc9c1\n+enqueued = await enqueue_pipeline(run_id, ...)\n+if not enqueued:\n+    background_tasks.add_task(_execute_pipeline, ...)\n+```\n+\n+## \uc124\uc815\n+\n+### \ud658\uacbd\ubcc0\uc218\n+\n+| \ubcc0\uc218 | \uae30\ubcf8\uac12 | \uc124\uba85 |\n+|------|--------|------|\n+| `REDIS_HOST` | `localhost` | Redis \ud638\uc2a4\ud2b8 |\n+| `REDIS_PORT` | `6379` | Redis \ud3ec\ud2b8 |\n+| `REDIS_DB` | `0` | Redis DB \ubc88\ud638 |\n+| `REDIS_PASSWORD` | `\"\"` | Redis \ube44\ubc00\ubc88\ud638 |\n+| `WORKER_MAX_JOBS` | `5` | \ub3d9\uc2dc \uc2e4\ud589 \ucd5c\ub300 \uc791\uc5c5 \uc218 |\n+| `WORKER_JOB_TIMEOUT` | `1800` | \uc791\uc5c5 \ud0c0\uc784\uc544\uc6c3 (\ucd08, \uae30\ubcf8 30\ubd84) |\n+| `WORKER_QUEUE_NAME` | `yaa:pipeline` | \ud050 \uc774\ub984 |\n+\n+### .env \uc608\uc2dc\n+\n+```bash\n+REDIS_HOST=localhost\n+REDIS_PORT=6379\n+WORKER_MAX_JOBS=3\n+```\n+\n+## \uc2e4\ud589 \ubc29\ubc95\n+\n+### Docker Compose (\uad8c\uc7a5)\n+\n+```bash\n+# \uc804\uccb4 \uc2a4\ud0dd (API + Worker + Redis + DB)\n+docker compose up -d\n+\n+# \ub85c\uadf8 \ud655\uc778\n+docker compose logs -f worker\n+```\n+\n+### \ub85c\uceec \uac1c\ubc1c\n+\n+```bash\n+# 1. Redis \uc2dc\uc791\n+make redis-up\n+\n+# 2. \uc6cc\ucee4 \uc2e4\ud589 (\ubcc4\ub3c4 \ud130\ubbf8\ub110)\n+make worker\n+\n+# 3. API \uc11c\ubc84 \uc2e4\ud589 (\ubcc4\ub3c4 \ud130\ubbf8\ub110)\n+make server\n+```\n+\n+## \ud30c\uc77c \uad6c\uc870\n+\n+```\n+packages/agents/src/worker/\n+\u251c\u2500\u2500 __init__.py      # \ubaa8\ub4c8 \ucd08\uae30\ud654\n+\u251c\u2500\u2500 config.py        # WorkerSettings (\ud658\uacbd\ubcc0\uc218 \uae30\ubc18)\n+\u251c\u2500\u2500 enqueue.py       # enqueue_pipeline() \ud5ec\ud37c\n+\u2514\u2500\u2500 tasks.py         # Arq \uc791\uc5c5 \uc815\uc758 + WorkerConfig\n+```\n+\n+## \uc2a4\ucf00\uc77c\ub9c1\n+\n+Worker \uc778\uc2a4\ud134\uc2a4\ub97c \ub298\ub824 \ubcd1\ub82c \ucc98\ub9ac\ub7c9\uc744 \uc99d\uac00\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n+\n+```bash\n+docker compose up -d --scale worker=3\n+```\n+\n+\uac01 Worker\ub294 `WORKER_MAX_JOBS`\uac1c\uc758 \uc791\uc5c5\uc744 \ub3d9\uc2dc\uc5d0 \ucc98\ub9ac\ud569\ub2c8\ub2e4.\ndiff --git a/packages/agents/pyproject.toml b/packages/agents/pyproject.toml\nindex bca5a2f..82aea3d 100644\n--- a/packages/agents/pyproject.toml\n+++ b/packages/agents/pyproject.toml\n@@ -56,6 +56,10 @@ auth = [\n rag = [\n     \"chromadb>=0.5\",\n ]\n+queue = [\n+    \"arq>=0.26\",\n+    \"redis>=5.0\",\n+]\n dashboard = []\n dev = [\n     \"pytest>=8.0.0\",\ndiff --git a/packages/agents/src/api/main.py b/packages/agents/src/api/main.py\nindex d40f95f..a8300b9 100644\n--- a/packages/agents/src/api/main.py\n+++ b/packages/agents/src/api/main.py\n@@ -28,6 +28,14 @@ async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:\n \n     yield\n \n+    # Arq Redis \ud480 \uc815\ub9ac\n+    try:\n+        from src.worker.enqueue import close_arq_pool\n+\n+        await close_arq_pool()\n+    except ImportError:\n+        pass\n+\n     logger.info(\"\uc560\ud50c\ub9ac\ucf00\uc774\uc158 \uc885\ub8cc\")\n \n \ndiff --git a/packages/agents/src/api/routes/pipeline.py b/packages/agents/src/api/routes/pipeline.py\nindex d5cc2f9..4c5d784 100644\n--- a/packages/agents/src/api/routes/pipeline.py\n+++ b/packages/agents/src/api/routes/pipeline.py\n@@ -36,11 +36,11 @@ async def _execute_pipeline(\n     settings: AppSettings,\n     channel_registry: ChannelRegistry,\n ) -> None:\n-    \"\"\"\ubc31\uadf8\ub77c\uc6b4\ub4dc\uc5d0\uc11c \ud30c\uc774\ud504\ub77c\uc778\uc744 \uc2e4\ud589\ud569\ub2c8\ub2e4.\"\"\"\n+    \"\"\"\ubc31\uadf8\ub77c\uc6b4\ub4dc\uc5d0\uc11c \ud30c\uc774\ud504\ub77c\uc778\uc744 \uc2e4\ud589\ud569\ub2c8\ub2e4 (Redis \ubbf8\uc0ac\uc6a9 \uc2dc \ud3f4\ubc31).\"\"\"\n     from src.cli import _build_agent_registry\n     from src.orchestrator import compile_pipeline, create_initial_state\n \n-    logger.info(\"\ud30c\uc774\ud504\ub77c\uc778 \uc2dc\uc791: run_id=%s, channel=%s\", run_id, channel_id)\n+    logger.info(\"\ud30c\uc774\ud504\ub77c\uc778 \uc2dc\uc791 (BackgroundTask \ud3f4\ubc31): run_id=%s, channel=%s\", run_id, channel_id)\n \n     session_factory = get_session_factory()\n     if session_factory is None:\n@@ -121,7 +121,11 @@ async def run_pipeline(\n     session: AsyncSession = Depends(get_db_session),\n     _api_key_id: str | None = Depends(require_api_key),\n ) -> PipelineRunResponse:\n-    \"\"\"\ud30c\uc774\ud504\ub77c\uc778\uc744 \ubc31\uadf8\ub77c\uc6b4\ub4dc\uc5d0\uc11c \uc2e4\ud589\ud569\ub2c8\ub2e4.\"\"\"\n+    \"\"\"\ud30c\uc774\ud504\ub77c\uc778\uc744 \uc2e4\ud589\ud569\ub2c8\ub2e4.\n+\n+    Redis\uac00 \uc0ac\uc6a9 \uac00\ub2a5\ud558\uba74 Arq \ud050\ub97c \ud1b5\ud574 \uc6cc\ucee4\uc5d0\uc11c \uc2e4\ud589\ud558\uace0,\n+    Redis\uac00 \uc5c6\uc73c\uba74 FastAPI BackgroundTasks\ub85c \ud3f4\ubc31\ud569\ub2c8\ub2e4.\n+    \"\"\"\n     run_id = str(uuid.uuid4())\n \n     repo = RunRepository(session)\n@@ -133,16 +137,33 @@ async def run_pipeline(\n         dry_run=request.dry_run,\n     )\n \n-    background_tasks.add_task(\n-        _execute_pipeline,\n-        run_id=run_id,\n-        channel_id=request.channel_id,\n-        topic=request.topic,\n-        brand_name=request.brand_name,\n-        dry_run=request.dry_run,\n-        settings=settings,\n-        channel_registry=channel_registry,\n-    )\n+    # Redis \ud050\ub85c enqueue \uc2dc\ub3c4\n+    enqueued = False\n+    try:\n+        from src.worker.enqueue import enqueue_pipeline\n+\n+        enqueued = await enqueue_pipeline(\n+            run_id=run_id,\n+            channel_id=request.channel_id,\n+            topic=request.topic,\n+            brand_name=request.brand_name,\n+            dry_run=request.dry_run,\n+        )\n+    except ImportError:\n+        logger.debug(\"arq \ud328\ud0a4\uc9c0 \ubbf8\uc124\uce58 \u2014 BackgroundTasks \ud3f4\ubc31\")\n+\n+    # \ud050 \ub4f1\ub85d \uc2e4\ud328 \uc2dc BackgroundTasks \ud3f4\ubc31\n+    if not enqueued:\n+        background_tasks.add_task(\n+            _execute_pipeline,\n+            run_id=run_id,\n+            channel_id=request.channel_id,\n+            topic=request.topic,\n+            brand_name=request.brand_name,\n+            dry_run=request.dry_run,\n+            settings=settings,\n+            channel_registry=channel_registry,\n+        )\n \n     return PipelineRunResponse(\n         run_id=run_id,\ndiff --git a/packages/agents/src/worker/__init__.py b/packages/agents/src/worker/__init__.py\nnew file mode 100644\nindex 0000000..3689bad\n--- /dev/null\n+++ b/packages/agents/src/worker/__init__.py\n@@ -0,0 +1 @@\n+\"\"\"\ube44\ub3d9\uae30 \uc791\uc5c5 \ud050 \uc6cc\ucee4 \ubaa8\ub4c8.\"\"\"\ndiff --git a/packages/agents/src/worker/config.py b/packages/agents/src/worker/config.py\nnew file mode 100644\nindex 0000000..1cbb164\n--- /dev/null\n+++ b/packages/agents/src/worker/config.py\n@@ -0,0 +1,37 @@\n+\"\"\"\uc6cc\ucee4 \ubc0f Redis \uc124\uc815.\"\"\"\n+\n+from __future__ import annotations\n+\n+from pydantic_settings import BaseSettings\n+\n+\n+class WorkerSettings(BaseSettings):\n+    \"\"\"Arq \uc6cc\ucee4 \uc124\uc815.\"\"\"\n+\n+    redis_host: str = \"localhost\"\n+    redis_port: int = 6379\n+    redis_db: int = 0\n+    redis_password: str = \"\"\n+\n+    worker_max_jobs: int = 5\n+    worker_job_timeout: int = 1800  # 30\ubd84\n+    worker_queue_name: str = \"yaa:pipeline\"\n+\n+    model_config = {\"env_file\": \".env\", \"env_file_encoding\": \"utf-8\", \"env_prefix\": \"\"}\n+\n+    @property\n+    def redis_settings(self):\n+        \"\"\"Arq RedisSettings \uac1d\uccb4\ub97c \ubc18\ud658\ud569\ub2c8\ub2e4.\"\"\"\n+        from arq.connections import RedisSettings as ArqRedisSettings\n+\n+        return ArqRedisSettings(\n+            host=self.redis_host,\n+            port=self.redis_port,\n+            database=self.redis_db,\n+            password=self.redis_password or None,\n+        )\n+\n+\n+def get_worker_settings() -> WorkerSettings:\n+    \"\"\"\uc6cc\ucee4 \uc124\uc815 \uc2f1\uae00\ud134\uc744 \ubc18\ud658\ud569\ub2c8\ub2e4.\"\"\"\n+    return WorkerSettings()\ndiff --git a/packages/agents/src/worker/enqueue.py b/packages/agents/src/worker/enqueue.py\nnew file mode 100644\nindex 0000000..b36a0dd\n--- /dev/null\n+++ b/packages/agents/src/worker/enqueue.py\n@@ -0,0 +1,76 @@\n+\"\"\"\ud050 \uc791\uc5c5 enqueue \ud5ec\ud37c.\"\"\"\n+\n+from __future__ import annotations\n+\n+import logging\n+\n+logger = logging.getLogger(__name__)\n+\n+_arq_pool = None\n+\n+\n+async def get_arq_pool():\n+    \"\"\"Arq Redis \ucee4\ub125\uc158 \ud480\uc744 \ubc18\ud658\ud569\ub2c8\ub2e4 (lazy singleton).\"\"\"\n+    global _arq_pool\n+    if _arq_pool is not None:\n+        return _arq_pool\n+\n+    try:\n+        from arq import create_pool\n+\n+        from src.worker.config import get_worker_settings\n+\n+        settings = get_worker_settings()\n+        _arq_pool = await create_pool(settings.redis_settings)\n+        logger.info(\"Arq Redis \ud480 \uc0dd\uc131 \uc644\ub8cc\")\n+        return _arq_pool\n+    except Exception:\n+        logger.debug(\"Redis \uc5f0\uacb0 \uc2e4\ud328 \u2014 BackgroundTasks \ud3f4\ubc31 \uc0ac\uc6a9\")\n+        return None\n+\n+\n+async def close_arq_pool() -> None:\n+    \"\"\"Arq Redis \ud480\uc744 \uc885\ub8cc\ud569\ub2c8\ub2e4.\"\"\"\n+    global _arq_pool\n+    if _arq_pool is not None:\n+        _arq_pool.close()\n+        await _arq_pool.wait_closed()\n+        _arq_pool = None\n+        logger.info(\"Arq Redis \ud480 \uc885\ub8cc\")\n+\n+\n+async def enqueue_pipeline(\n+    run_id: str,\n+    channel_id: str,\n+    topic: str,\n+    brand_name: str,\n+    dry_run: bool,\n+) -> bool:\n+    \"\"\"\ud30c\uc774\ud504\ub77c\uc778 \uc2e4\ud589\uc744 \ud050\uc5d0 \ub4f1\ub85d\ud569\ub2c8\ub2e4.\n+\n+    Returns:\n+        True: \ud050\uc5d0 \uc131\uacf5\uc801\uc73c\ub85c \ub4f1\ub85d\ub428\n+        False: Redis \ubbf8\uc0ac\uc6a9 \ub610\ub294 \uc5f0\uacb0 \uc2e4\ud328 (\ud3f4\ubc31 \ud544\uc694)\n+    \"\"\"\n+    pool = await get_arq_pool()\n+    if pool is None:\n+        return False\n+\n+    try:\n+        from src.worker.config import get_worker_settings\n+\n+        settings = get_worker_settings()\n+        await pool.enqueue_job(\n+            \"execute_pipeline_task\",\n+            run_id=run_id,\n+            channel_id=channel_id,\n+            topic=topic,\n+            brand_name=brand_name,\n+            dry_run=dry_run,\n+            _queue_name=settings.worker_queue_name,\n+        )\n+        logger.info(\"\ud30c\uc774\ud504\ub77c\uc778 \ud050 \ub4f1\ub85d: run_id=%s\", run_id)\n+        return True\n+    except Exception:\n+        logger.warning(\"\ud050 \ub4f1\ub85d \uc2e4\ud328, \ud3f4\ubc31 \ud544\uc694: run_id=%s\", run_id, exc_info=True)\n+        return False\ndiff --git a/packages/agents/src/worker/tasks.py b/packages/agents/src/worker/tasks.py\nnew file mode 100644\nindex 0000000..b1135f8\n--- /dev/null\n+++ b/packages/agents/src/worker/tasks.py\n@@ -0,0 +1,160 @@\n+\"\"\"Arq \uc791\uc5c5 \uc815\uc758 \ubc0f \uc6cc\ucee4 \uc9c4\uc785\uc810.\"\"\"\n+\n+from __future__ import annotations\n+\n+import logging\n+import uuid\n+from typing import Any\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def _save_usage_events(\n+    session_factory: Any,\n+    collector: Any,\n+    run_id: str,\n+) -> None:\n+    \"\"\"\uc218\uc9d1\ub41c LLM \uc0ac\uc6a9\ub7c9 \uc774\ubca4\ud2b8\ub97c DB\uc5d0 \uc800\uc7a5\ud569\ub2c8\ub2e4.\"\"\"\n+    if not collector.events:\n+        return\n+\n+    try:\n+        from src.database.repositories import UsageRepository\n+\n+        async with session_factory() as session:\n+            usage_repo = UsageRepository(session)\n+            for event in collector.events:\n+                await usage_repo.create(\n+                    event_id=str(uuid.uuid4()),\n+                    run_id=run_id,\n+                    **event,\n+                )\n+            await session.commit()\n+        logger.info(\n+            \"\uc0ac\uc6a9\ub7c9 \uc774\ubca4\ud2b8 \uc800\uc7a5: run_id=%s, count=%d\", run_id, len(collector.events)\n+        )\n+    except Exception:\n+        logger.exception(\"\uc0ac\uc6a9\ub7c9 \uc774\ubca4\ud2b8 \uc800\uc7a5 \uc2e4\ud328: run_id=%s\", run_id)\n+\n+\n+async def execute_pipeline_task(\n+    ctx: dict[str, Any],\n+    run_id: str,\n+    channel_id: str,\n+    topic: str,\n+    brand_name: str,\n+    dry_run: bool,\n+) -> dict[str, Any]:\n+    \"\"\"\ud050\uc5d0\uc11c \ud30c\uc774\ud504\ub77c\uc778\uc744 \uc2e4\ud589\ud558\ub294 Arq \uc791\uc5c5.\n+\n+    Args:\n+        ctx: Arq \ucee8\ud14d\uc2a4\ud2b8 (startup\uc5d0\uc11c \uc8fc\uc785\ud55c \uc758\uc874\uc131 \ud3ec\ud568)\n+        run_id: \ud30c\uc774\ud504\ub77c\uc778 \uc2e4\ud589 ID\n+        channel_id: \ucc44\ub110 ID\n+        topic: \ucf58\ud150\uce20 \uc8fc\uc81c\n+        brand_name: \ube0c\ub79c\ub4dc\uba85\n+        dry_run: \uc2e4\uc81c \uc5c5\ub85c\ub4dc \uac74\ub108\ub700 \uc5ec\ubd80\n+\n+    Returns:\n+        \uc2e4\ud589 \uacb0\uacfc \ub515\uc154\ub108\ub9ac\n+    \"\"\"\n+    from src.cli import _build_agent_registry\n+    from src.database.engine import get_session_factory\n+    from src.database.repositories import RunRepository\n+    from src.orchestrator import compile_pipeline, create_initial_state\n+    from src.shared.config import AppSettings\n+    from src.shared.llm_clients import UsageCollector\n+\n+    logger.info(\"\ud30c\uc774\ud504\ub77c\uc778 \uc791\uc5c5 \uc2dc\uc791: run_id=%s, channel=%s\", run_id, channel_id)\n+\n+    session_factory = get_session_factory()\n+    if session_factory is None:\n+        logger.error(\"DB \uc138\uc158 \ud329\ud1a0\ub9ac\uac00 \uc5c6\uc2b5\ub2c8\ub2e4: run_id=%s\", run_id)\n+        return {\"status\": \"error\", \"message\": \"DB \uc138\uc158 \ud329\ud1a0\ub9ac \uc5c6\uc74c\"}\n+\n+    # running \uc0c1\ud0dc\ub85c \uc5c5\ub370\uc774\ud2b8\n+    async with session_factory() as session:\n+        repo = RunRepository(session)\n+        await repo.update_status(run_id, status=\"running\")\n+        await session.commit()\n+\n+    collector = UsageCollector()\n+\n+    try:\n+        settings = AppSettings()\n+        channel_registry = ctx.get(\"channel_registry\")\n+        if channel_registry is None:\n+            from src.shared.config import ChannelRegistry\n+\n+            channel_registry = ChannelRegistry(settings.channels_dir)\n+\n+        agent_registry = _build_agent_registry(settings, collector=collector)\n+        pipeline = compile_pipeline(agent_registry)\n+        initial_state = create_initial_state(\n+            channel_id=channel_id,\n+            topic=topic,\n+            brand_name=brand_name,\n+            dry_run=dry_run,\n+        )\n+\n+        final_state = await pipeline.ainvoke(initial_state)\n+\n+        result: dict[str, Any] = {\n+            \"content_status\": str(final_state.get(\"status\", \"\")),\n+            \"errors\": final_state.get(\"errors\", []),\n+        }\n+\n+        async with session_factory() as session:\n+            repo = RunRepository(session)\n+            await repo.update_status(run_id, status=\"completed\", result=result)\n+            await session.commit()\n+\n+        logger.info(\"\ud30c\uc774\ud504\ub77c\uc778 \uc644\ub8cc: run_id=%s\", run_id)\n+        return {\"status\": \"completed\", \"run_id\": run_id}\n+\n+    except Exception as exc:\n+        logger.exception(\"\ud30c\uc774\ud504\ub77c\uc778 \uc2e4\ud328: run_id=%s\", run_id)\n+        async with session_factory() as session:\n+            repo = RunRepository(session)\n+            await repo.update_status(run_id, status=\"failed\", errors=[str(exc)])\n+            await session.commit()\n+        return {\"status\": \"failed\", \"run_id\": run_id, \"error\": str(exc)}\n+\n+    finally:\n+        await _save_usage_events(session_factory, collector, run_id)\n+\n+\n+async def startup(ctx: dict[str, Any]) -> None:\n+    \"\"\"\uc6cc\ucee4 \uc2dc\uc791 \uc2dc \ub9ac\uc18c\uc2a4\ub97c \ucd08\uae30\ud654\ud569\ub2c8\ub2e4.\"\"\"\n+    from src.database.engine import init_db\n+    from src.shared.config import AppSettings, ChannelRegistry\n+\n+    settings = AppSettings()\n+    await init_db(settings.database_url)\n+\n+    ctx[\"channel_registry\"] = ChannelRegistry(settings.channels_dir)\n+    logger.info(\"\uc6cc\ucee4 \ucd08\uae30\ud654 \uc644\ub8cc\")\n+\n+\n+async def shutdown(ctx: dict[str, Any]) -> None:\n+    \"\"\"\uc6cc\ucee4 \uc885\ub8cc \uc2dc \ub9ac\uc18c\uc2a4\ub97c \uc815\ub9ac\ud569\ub2c8\ub2e4.\"\"\"\n+    logger.info(\"\uc6cc\ucee4 \uc885\ub8cc\")\n+\n+\n+class WorkerConfig:\n+    \"\"\"Arq \uc6cc\ucee4 \uc124\uc815 \ud074\ub798\uc2a4.\n+\n+    arq CLI\uc5d0\uc11c `arq src.worker.tasks.WorkerConfig` \uc73c\ub85c \uc2e4\ud589\ud569\ub2c8\ub2e4.\n+    \"\"\"\n+\n+    from src.worker.config import get_worker_settings\n+\n+    _settings = get_worker_settings()\n+\n+    functions = [execute_pipeline_task]\n+    on_startup = startup\n+    on_shutdown = shutdown\n+    redis_settings = _settings.redis_settings\n+    max_jobs = _settings.worker_max_jobs\n+    job_timeout = _settings.worker_job_timeout\n+    queue_name = _settings.worker_queue_name\n",
  "test_patch": "diff --git a/packages/agents/tests/test_worker.py b/packages/agents/tests/test_worker.py\nnew file mode 100644\nindex 0000000..8c64ba1\n--- /dev/null\n+++ b/packages/agents/tests/test_worker.py\n@@ -0,0 +1,279 @@\n+\"\"\"\ube44\ub3d9\uae30 \uc791\uc5c5 \ud050 \uc6cc\ucee4 \ud14c\uc2a4\ud2b8.\"\"\"\n+\n+from __future__ import annotations\n+\n+from unittest.mock import AsyncMock, MagicMock, patch\n+\n+import pytest\n+\n+# ============================================\n+# Worker Config \ud14c\uc2a4\ud2b8\n+# ============================================\n+\n+\n+class TestWorkerConfig:\n+    \"\"\"\uc6cc\ucee4 \uc124\uc815 \ud14c\uc2a4\ud2b8.\"\"\"\n+\n+    def test_\uae30\ubcf8_\uc124\uc815_\uac12(self):\n+        from src.worker.config import WorkerSettings\n+\n+        settings = WorkerSettings()\n+        assert settings.redis_host == \"localhost\"\n+        assert settings.redis_port == 6379\n+        assert settings.redis_db == 0\n+        assert settings.worker_max_jobs == 5\n+        assert settings.worker_job_timeout == 1800\n+        assert settings.worker_queue_name == \"yaa:pipeline\"\n+\n+    def test_\ud658\uacbd\ubcc0\uc218_\uc624\ubc84\ub77c\uc774\ub4dc(self, monkeypatch):\n+        from src.worker.config import WorkerSettings\n+\n+        monkeypatch.setenv(\"REDIS_HOST\", \"redis.example.com\")\n+        monkeypatch.setenv(\"REDIS_PORT\", \"6380\")\n+        monkeypatch.setenv(\"REDIS_DB\", \"2\")\n+        monkeypatch.setenv(\"WORKER_MAX_JOBS\", \"10\")\n+\n+        settings = WorkerSettings()\n+        assert settings.redis_host == \"redis.example.com\"\n+        assert settings.redis_port == 6380\n+        assert settings.redis_db == 2\n+        assert settings.worker_max_jobs == 10\n+\n+    def test_redis_settings_\ud504\ub85c\ud37c\ud2f0(self):\n+        from src.worker.config import WorkerSettings\n+\n+        settings = WorkerSettings()\n+        redis_settings = settings.redis_settings\n+        assert redis_settings.host == \"localhost\"\n+        assert redis_settings.port == 6379\n+\n+    def test_get_worker_settings(self):\n+        from src.worker.config import get_worker_settings\n+\n+        settings = get_worker_settings()\n+        assert settings.redis_host == \"localhost\"\n+\n+\n+# ============================================\n+# Enqueue \ud14c\uc2a4\ud2b8\n+# ============================================\n+\n+\n+class TestEnqueue:\n+    \"\"\"\ud050 \ub4f1\ub85d \ud5ec\ud37c \ud14c\uc2a4\ud2b8.\"\"\"\n+\n+    @pytest.fixture(autouse=True)\n+    def _reset_pool(self):\n+        \"\"\"\uac01 \ud14c\uc2a4\ud2b8 \uc804\uc5d0 \uae00\ub85c\ubc8c \ud480\uc744 \ub9ac\uc14b\ud569\ub2c8\ub2e4.\"\"\"\n+        import src.worker.enqueue as mod\n+\n+        mod._arq_pool = None\n+        yield\n+        mod._arq_pool = None\n+\n+    async def test_redis_\ubbf8\uc5f0\uacb0\uc2dc_false_\ubc18\ud658(self):\n+        from src.worker.enqueue import enqueue_pipeline\n+\n+        result = await enqueue_pipeline(\n+            run_id=\"test-run\",\n+            channel_id=\"test-ch\",\n+            topic=\"topic\",\n+            brand_name=\"brand\",\n+            dry_run=True,\n+        )\n+        assert result is False\n+\n+    async def test_redis_\uc5f0\uacb0_\uc131\uacf5\uc2dc_true_\ubc18\ud658(self):\n+        import src.worker.enqueue as mod\n+\n+        mock_pool = AsyncMock()\n+        mock_pool.enqueue_job = AsyncMock()\n+        mod._arq_pool = mock_pool\n+\n+        from src.worker.enqueue import enqueue_pipeline\n+\n+        result = await enqueue_pipeline(\n+            run_id=\"test-run\",\n+            channel_id=\"test-ch\",\n+            topic=\"topic\",\n+            brand_name=\"brand\",\n+            dry_run=True,\n+        )\n+        assert result is True\n+        mock_pool.enqueue_job.assert_called_once()\n+\n+    async def test_enqueue_\uc2e4\ud328\uc2dc_false_\ubc18\ud658(self):\n+        import src.worker.enqueue as mod\n+\n+        mock_pool = AsyncMock()\n+        mock_pool.enqueue_job = AsyncMock(side_effect=ConnectionError(\"Redis \uc5f0\uacb0 \ub04a\uae40\"))\n+        mod._arq_pool = mock_pool\n+\n+        from src.worker.enqueue import enqueue_pipeline\n+\n+        result = await enqueue_pipeline(\n+            run_id=\"test-run\",\n+            channel_id=\"test-ch\",\n+            topic=\"topic\",\n+            brand_name=\"brand\",\n+            dry_run=True,\n+        )\n+        assert result is False\n+\n+    async def test_close_arq_pool(self):\n+        import src.worker.enqueue as mod\n+\n+        mock_pool = MagicMock()\n+        mock_pool.close = MagicMock()\n+        mock_pool.wait_closed = AsyncMock()\n+        mod._arq_pool = mock_pool\n+\n+        from src.worker.enqueue import close_arq_pool\n+\n+        await close_arq_pool()\n+        mock_pool.close.assert_called_once()\n+        mock_pool.wait_closed.assert_called_once()\n+        assert mod._arq_pool is None\n+\n+    async def test_close_arq_pool_none\uc774\uba74_\ubb34\uc2dc(self):\n+        from src.worker.enqueue import close_arq_pool\n+\n+        # None\uc77c \ub54c \uc5d0\ub7ec \uc5c6\uc774 \ud1b5\uacfc\ud574\uc57c \ud568\n+        await close_arq_pool()\n+\n+\n+# ============================================\n+# Pipeline API \ud050 \ud1b5\ud569 \ud14c\uc2a4\ud2b8\n+# ============================================\n+\n+\n+class TestPipelineQueueIntegration:\n+    \"\"\"\ud30c\uc774\ud504\ub77c\uc778 API\uc5d0\uc11c \ud050 enqueue\ub97c \uc0ac\uc6a9\ud558\ub294 \ud1b5\ud569 \ud14c\uc2a4\ud2b8.\"\"\"\n+\n+    @pytest.fixture()\n+    async def _db_session_factory(self):\n+        from src.database.engine import init_db, set_session_factory\n+\n+        factory = await init_db(\"sqlite+aiosqlite:///:memory:\")\n+        yield factory\n+        set_session_factory(None)\n+\n+    @pytest.fixture()\n+    def _channels_dir(self, tmp_path):\n+        ch_dir = tmp_path / \"channels\"\n+        ch_dir.mkdir()\n+        ch = ch_dir / \"test-channel\"\n+        ch.mkdir()\n+        (ch / \"config.yaml\").write_text(\n+            \"channel:\\n  name: '\ud14c\uc2a4\ud2b8 \ucc44\ub110'\\n  category: 'test'\\n\",\n+            encoding=\"utf-8\",\n+        )\n+        return ch_dir\n+\n+    @pytest.fixture()\n+    def client(self, _channels_dir, _db_session_factory):\n+        from fastapi.testclient import TestClient\n+\n+        from src.api.dependencies import get_channel_registry, get_settings\n+        from src.api.main import create_app\n+        from src.database.engine import get_db_session\n+        from src.shared.config import AppSettings, ChannelRegistry\n+\n+        app = create_app()\n+        registry = ChannelRegistry(str(_channels_dir))\n+        test_settings = AppSettings(\n+            disable_auth=True,\n+            database_url=\"sqlite+aiosqlite:///:memory:\",\n+            channels_dir=str(_channels_dir),\n+        )\n+        app.dependency_overrides[get_channel_registry] = lambda: registry\n+        app.dependency_overrides[get_settings] = lambda: test_settings\n+\n+        async def _override_db_session():\n+            async with _db_session_factory() as session:\n+                try:\n+                    yield session\n+                    await session.commit()\n+                except Exception:\n+                    await session.rollback()\n+                    raise\n+\n+        app.dependency_overrides[get_db_session] = _override_db_session\n+\n+        with TestClient(app) as c:\n+            yield c\n+\n+    def test_redis_\uc5c6\uc744\ub54c_BackgroundTasks_\ud3f4\ubc31(self, client):\n+        \"\"\"Redis\uac00 \uc5c6\uc73c\uba74 BackgroundTasks\ub85c \ud3f4\ubc31\ud558\uc5ec \uc815\uc0c1 \uc751\ub2f5.\"\"\"\n+        response = client.post(\n+            \"/api/v1/pipeline/run\",\n+            json={\n+                \"channel_id\": \"test-channel\",\n+                \"topic\": \"\ud050 \ud14c\uc2a4\ud2b8\",\n+                \"dry_run\": True,\n+            },\n+        )\n+        assert response.status_code == 200\n+        data = response.json()\n+        assert data[\"status\"] == \"pending\"\n+        assert \"run_id\" in data\n+\n+    @patch(\"src.worker.enqueue.enqueue_pipeline\", new_callable=AsyncMock)\n+    def test_redis_\uc788\uc744\ub54c_\ud050\ub85c_\uc804\ub2ec(self, mock_enqueue, client):\n+        \"\"\"Redis\uac00 \uac00\uc6a9\ud558\uba74 \ud050\uc5d0 enqueue\ub428.\"\"\"\n+        mock_enqueue.return_value = True\n+\n+        response = client.post(\n+            \"/api/v1/pipeline/run\",\n+            json={\n+                \"channel_id\": \"test-channel\",\n+                \"topic\": \"\ud050 enqueue \ud14c\uc2a4\ud2b8\",\n+                \"dry_run\": True,\n+            },\n+        )\n+        assert response.status_code == 200\n+        data = response.json()\n+        assert data[\"status\"] == \"pending\"\n+        mock_enqueue.assert_called_once()\n+\n+    @patch(\"src.worker.enqueue.enqueue_pipeline\", new_callable=AsyncMock)\n+    def test_enqueue_\uc2e4\ud328\uc2dc_\ud3f4\ubc31(self, mock_enqueue, client):\n+        \"\"\"enqueue \uc2e4\ud328 \uc2dc BackgroundTasks\ub85c \ud3f4\ubc31.\"\"\"\n+        mock_enqueue.return_value = False\n+\n+        response = client.post(\n+            \"/api/v1/pipeline/run\",\n+            json={\n+                \"channel_id\": \"test-channel\",\n+                \"topic\": \"\ud3f4\ubc31 \ud14c\uc2a4\ud2b8\",\n+                \"dry_run\": True,\n+            },\n+        )\n+        assert response.status_code == 200\n+        data = response.json()\n+        assert data[\"status\"] == \"pending\"\n+\n+\n+# ============================================\n+# Worker Task \ud14c\uc2a4\ud2b8\n+# ============================================\n+\n+\n+class TestExecutePipelineTask:\n+    \"\"\"execute_pipeline_task \uc720\ub2db \ud14c\uc2a4\ud2b8.\"\"\"\n+\n+    async def test_\uc138\uc158_\ud329\ud1a0\ub9ac_\uc5c6\uc73c\uba74_\uc5d0\ub7ec_\ubc18\ud658(self):\n+        from src.database.engine import set_session_factory\n+        from src.worker.tasks import execute_pipeline_task\n+\n+        set_session_factory(None)\n+\n+        result = await execute_pipeline_task(\n+            ctx={},\n+            run_id=\"test-run\",\n+            channel_id=\"test-ch\",\n+            topic=\"topic\",\n+            brand_name=\"brand\",\n+            dry_run=True,\n+        )\n+        assert result[\"status\"] == \"error\"\n",
  "hints_text": "",
  "environment_setup_commit": "43a8aab546ef1f289da9d5f91618bdf00b7ccc67",
  "install_config": {},
  "meta": {
    "commit_name": "head_commit",
    "num_modified_files": 11,
    "has_test_patch": true,
    "is_lite": false,
    "llm_score": {
      "difficulty_score": null,
      "issue_text_score": null,
      "test_score": null
    }
  },
  "license_name": "MIT",
  "FAIL_TO_PASS": [],
  "PASS_TO_PASS": [],
  "requirements": "",
  "environment": ""
}